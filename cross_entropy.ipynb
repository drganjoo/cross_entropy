{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy Method\n",
    "\n",
    "---\n",
    "\n",
    "This notebook, shows a sample implementation of Cross-Entropy Method with OpenAI Gym's MountainCarContinuous environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Description\n",
    "\n",
    "State space: Continous    \n",
    "Action Space: Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.seed(101)\n",
    "np.random.seed(101)\n",
    "\n",
    "print('observation space:', env.observation_space)\n",
    "print('observation space shape:', env.observation_space.shape)\n",
    "print('action space:', env.action_space)\n",
    "print('  - low:', env.action_space.low)\n",
    "print('  - high:', env.action_space.high)\n",
    "print('action space size:', env.action_space.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Agent Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class WeightModifier:\n",
    "    \"\"\"\n",
    "    Given a flat weights array, this class can disribute the array weights to individual\n",
    "    layer components.\n",
    "    \n",
    "    Total number of weights required can be found by calling get_weights_dim() method\n",
    "    \"\"\"\n",
    "    def _set_layers(self, layers: List[nn.Linear]):\n",
    "        \"\"\"\n",
    "        All layers of the network are saved in an array and indices are computed to be used\n",
    "        later to copy weights from a flat numpy array containing weights for all of the \n",
    "        layers.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.w_indices = []\n",
    "        idx_start = 0\n",
    "        for l in self.layers:\n",
    "            indices = self._w_b_indices(l, idx_start)\n",
    "            self.w_indices.append((idx_start, *indices))\n",
    "            idx_start = indices[1]\n",
    "            \n",
    "    def _w_b_indices(self, layer, start):\n",
    "        \"\"\"\n",
    "        returns the indices in the weights array where the given layer's \n",
    "        weights and biases are to be copied from\n",
    "        \"\"\"\n",
    "        w = np.prod(layer.weight.shape)   # e.g. 4 layer with 2 input would be (4,2)=8 shape\n",
    "        b = layer.bias.shape[0]           # bias only has as many as neurons in this layer\n",
    "        return start + w, start + w + b\n",
    "\n",
    "    def _set_layer_weights_(self, layer, weights, layer_no):\n",
    "        start, layer_w, layer_b = self.w_indices[layer_no]\n",
    "        \n",
    "        # pick up weights and biases from the weights array passed in\n",
    "        w = weights[start: layer_w]\n",
    "        b = weights[layer_w: layer_b]\n",
    "        \n",
    "        # change weight and bias of the given layer\n",
    "        layer.weight.data.copy_(torch.from_numpy(w).view(layer.weight.shape))\n",
    "        layer.bias.data.copy_(torch.from_numpy(b).view(layer.bias.shape))\n",
    "        \n",
    "        # returns the index where next layer's weights will start from\n",
    "        return layer_b\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        index = 0\n",
    "        for idx, l in enumerate(self.layers):\n",
    "            self._set_layer_weights_(l, weights, idx)\n",
    "        return self\n",
    "\n",
    "    def get_weights(self):\n",
    "        w = np.zeros(self.get_weights_dim())\n",
    "        for idx, l in enumerate(self.layers):\n",
    "            i = self.w_indices[idx]\n",
    "            w[i[0] : i[1]] = l.weight.data.cpu().detach().numpy().reshape(-1)\n",
    "            w[i[1] : i[2]] = l.bias.data.cpu().detach().numpy().reshape(-1)\n",
    "        return np.array(w)\n",
    "        \n",
    "    def get_weights_dim(self):\n",
    "        return self.w_indices[-1][2]\n",
    "    \n",
    "    def gen_random(self, sigma = 1.):\n",
    "        return sigma * np.random.rand(self.get_weights_dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module, WeightModifier):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        s_size = env.observation_space.shape[0]\n",
    "        a_size = env.action_space.shape[0]\n",
    "\n",
    "        self.fc1 = nn.Linear(s_size, 16)\n",
    "        self.fc2 = nn.Linear(16, a_size)\n",
    "        \n",
    "        super()._set_layers([self.fc1, self.fc2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_tensor = torch.from_numpy(x).float().to(self.fc1.weight.device)\n",
    "        output = F.relu(self.fc1(x_tensor))\n",
    "        output = torch.tanh(self.fc2(output))\n",
    "        return output.cpu().data\n",
    "    \n",
    "    def act(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "agent = Agent(env).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(agent, env, gamma=1.0, max_t=5000):\n",
    "    ep_reward = 0.0\n",
    "    gamma_t = 1\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for t in range(max_t):\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        score += reward\n",
    "        ep_reward += reward * gamma_t\n",
    "        gamma_t *= gamma\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return score, ep_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train Agent with the Cross-Entropy Method\n",
    "\n",
    "The following cell will train the agent using cross entropy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cem(agent, n_episodes=500, max_t=1000, gamma=1.0, print_every=10, pop_size=50, elite_frac=0.2, sigma=0.5):\n",
    "    \"\"\"PyTorch implementation of the cross-entropy method.\n",
    "        \n",
    "    Params\n",
    "    ======\n",
    "        n_iterations (int): maximum number of training iterations\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        gamma (float): discount rate\n",
    "        print_every (int): how often to print average score (over last 100 episodes)\n",
    "        pop_size (int): size of population at each iteration\n",
    "        elite_frac (float): percentage of top performers to use in update\n",
    "        sigma (float): standard deviation of additive noise\n",
    "    \"\"\"\n",
    "    n_elite = int(pop_size * elite_frac)\n",
    "\n",
    "    scores_100 = deque(maxlen=100)\n",
    "    scores = []\n",
    "    dim = agent.get_weights_dim()\n",
    "    best_weight = sigma * np.random.randn(dim)\n",
    "\n",
    "    agent_with_weight = lambda a, w: a.set_weights(w)\n",
    "    \n",
    "    for ep_no in range(1, n_episodes + 1):\n",
    "        # generate 50 more populations by adding small random numbers\n",
    "        # to the best weights that we have with us\n",
    "        \n",
    "        weights_pop = best_weight + sigma * np.random.randn(pop_size, dim)\n",
    "\n",
    "        # run each of these 50 population through the agent and get their rewards\n",
    "        rewards = []\n",
    "        for i in range(pop_size):\n",
    "            print(f'\\rEp: {ep_no} Population: {i}', end='')\n",
    "            _, r = run_episode(agent_with_weight(agent, weights_pop[i]), env)\n",
    "            rewards.append(r)\n",
    "\n",
    "        # pick the best rewarding weight indices from the rewards and then pick the corresponding weights\n",
    "        pop_best_idx = np.array(rewards).argsort()[-n_elite:]\n",
    "        pop_best_weights = weights_pop[pop_best_idx]\n",
    "        \n",
    "        # use the mean of the best indices\n",
    "        best_weight = pop_best_weights.mean(axis=0)\n",
    "\n",
    "        # evaluate the new weights and keep their scroes in the last 100 episode array\n",
    "        best_agent = agent_with_weight(agent, best_weight)\n",
    "        \n",
    "        _, reward = run_episode(agent, env, gamma = 1.)\n",
    "        scores_100.append(reward)\n",
    "        scores.append(reward)\n",
    "        \n",
    "        torch.save(agent.state_dict(), 'checkpoint2.pth')\n",
    "        \n",
    "        if ep_no % print_every == 0:\n",
    "            print(f'\\rEpisode {ep_no}\\tAverage Score: {np.mean(scores_100):.2f}', end='')\n",
    "\n",
    "        if np.mean(scores_100) >= 90.0:\n",
    "            print(f'\\nEnv solved in {ep_no:d} iterations!\\tAverage Score: {np.mean(scores_100):.2f}')\n",
    "            break\n",
    "            \n",
    "    return scores\n",
    "\n",
    "scores = cem(agent, n_episodes = 500)\n",
    "print('\\nFinished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch a Smart Agent!\n",
    "\n",
    "In the next code cell, you will load the trained weights from file to watch a smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "agent.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "state = env.reset()\n",
    "while True:\n",
    "    state = torch.from_numpy(state).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        action = agent(state)\n",
    "    env.render()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
